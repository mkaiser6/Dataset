{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZYXwtCsL_y"
   },
   "source": [
    "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_DvtT7sOIr"
   },
   "outputs": [],
   "source": [
    "import os # Miscellaneous operating system interfaces, provides a portable way of using operating system dependent functionality\n",
    "import numpy as np # Numerical Python, to work with large, multi-dimensional arrays and matrices,high-level mathematical functions to operate on arrays\n",
    "from collections import Counter #dict subclass for counting hashable objects\n",
    "from sklearn.naive_bayes import GaussianNB #implements the Gaussian Naive Bayes algorithm for classification\n",
    "from sklearn.metrics import accuracy_score  #multilabel classification, computes subset accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXknSIrLvzfQ"
   },
   "source": [
    "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjKF0nIMwz8_"
   },
   "outputs": [],
   "source": [
    "#Function that reads the email files from a folder and constructs a dictionary for all words\n",
    "\n",
    "def make_dict(main_dir): # defining the function, with parameter \"main_dir\"\n",
    "  all_words = [] # creating empty list \n",
    "  emails = [os.path.join(main_dir,f) for f in os.listdir(main_dir)] # creating a list 'emails'\n",
    "  \n",
    "  # os.path.join() method returns a string which represents the concatenated path components\n",
    "  # os.listdir() method retrieves a list of all the files in 'main_dir'\n",
    "  # for loop through all emails in emails\n",
    "    \n",
    "  for mail in emails:\n",
    "    with open(mail) as p: #'with' simplifies management of file streams, no need to call file.close(), save as 'p'\n",
    "      for line in p:  # loop through each item in email \n",
    "        words = line.split()  # returns a list of all the words in the string\n",
    "        all_words += words # creating list with all words (addition)\n",
    "  dictionary = Counter(all_words)  # creates dictionary with key + value (keeps track of how many times equivalent values are added)\n",
    "  list_to_remove = list(dictionary) # returns a list from dictionary \n",
    "\n",
    "  for item in list_to_remove: # iterating over the list \n",
    "    if item.isalpha() == False:  # deleting if not alphabet letter (a-z), isaplha()- boolean True/False\n",
    "      del dictionary[item]\n",
    "    elif len(item) == 1: # deleting if only 1 character (lenght of word = 1)\n",
    "      del dictionary[item]\n",
    "  dictionary = dictionary.most_common(3000)  # returns list,sorted based on the count of the elements, limit 3000 and saving back in dictionary\n",
    "  return dictionary\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_NG2-TpxQ1j"
   },
   "source": [
    "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmVW5xNlyOFc"
   },
   "outputs": [],
   "source": [
    "# extract features function that removes less common words from every document \n",
    "\n",
    "def extract_feat(email_dir): # defining fucntion with parameter 'email_dir'\n",
    "  documents = [os.path.join(email_dir,fi) for fi in os.listdir(email_dir)]   # create list with joined path components\n",
    "  features_matrix = np.zeros((len(documents),3000))  # Return a new array (Matrix) of given shape(len of documents rows) and columns 3000 (and optional type), filled with zeros\n",
    "  train_labels = np.zeros(len(documents)) # zeros() function --> matrix with 0 with give shape (len of files)\n",
    "  count = 1;  # start of counter set to 1\n",
    "  docID = 0; #docId set to 0 \n",
    "  for fil in documents: # iterate over each file \n",
    "    with open(fil) as fi: # save as fi \n",
    "      for i, line in enumerate(fi):  # enumerate to get counter in loop\n",
    "        if i ==2:\n",
    "          words = line.split() # split if 2 \n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')  # use split method at each character backslash ('/') character \n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    "      if lastToken.startswith(\"spmsg\"):  #  we know that spmsga162.txt files are spam \n",
    "        train_labels[docID] = 1;  # label as 1 (spam mail), dummy variable?!\n",
    "        count = count + 1  # increaese by 1 for each loop\n",
    "      docID = docID + 1\n",
    "  return features_matrix, train_labels                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbOV1Y4hxpiy"
   },
   "source": [
    "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoq-rE7Mx0pp"
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = '/Users/maxkaiser/Desktop/train-mails' # file path, download local \n",
    "TEST_DIR = '/Users/maxkaiser/Desktop/test-mails' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n",
      "Training Model using Gaussian Naibe Bayes algorithm .....\n",
      "Training completed\n",
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
      "0.9653846153846154\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_dict(TRAIN_DIR)\n",
    "\n",
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "features_matrix, labels = extract_feat(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_feat(TEST_DIR)\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
    "# train model\n",
    "model.fit(features_matrix, labels)\n",
    "print (\"Training completed\")\n",
    "print (\"testing trained model to predict Test Data labels\")\n",
    "#predict/test\n",
    "predicted_labels = model.predict(test_features_matrix)\n",
    "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
    "print (accuracy_score(test_labels, predicted_labels))\n",
    "# percentage of correct predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
